{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_grammar_error_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9564d91b31584ebfbc3f304de49b3649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1005db29a16544bca890947e2cdda77a",
              "IPY_MODEL_7dfddb72204545ca809d76db9cb3cca2",
              "IPY_MODEL_d39864d43a6a4a0eb5c698eb79b0e45c"
            ],
            "layout": "IPY_MODEL_a331b1142d004d2ca8d8cbf49603fd33"
          }
        },
        "1005db29a16544bca890947e2cdda77a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3632c3540f6f4f398387a675efaf5aed",
            "placeholder": "​",
            "style": "IPY_MODEL_bfc279b71e2c4eedbec2b59abac9f968",
            "value": "Downloading: 100%"
          }
        },
        "7dfddb72204545ca809d76db9cb3cca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_481f63f6c7ee4c719e0760c93a3d3bcb",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b07367a3ee240c7a92f372d55d8ee92",
            "value": 440473133
          }
        },
        "d39864d43a6a4a0eb5c698eb79b0e45c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b0f81ab4a4348d7bf58d882c4cbaf7a",
            "placeholder": "​",
            "style": "IPY_MODEL_711c01a605a34ed9b15aa0aea6225d7a",
            "value": " 420M/420M [00:13&lt;00:00, 26.8MB/s]"
          }
        },
        "a331b1142d004d2ca8d8cbf49603fd33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3632c3540f6f4f398387a675efaf5aed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfc279b71e2c4eedbec2b59abac9f968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "481f63f6c7ee4c719e0760c93a3d3bcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b07367a3ee240c7a92f372d55d8ee92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b0f81ab4a4348d7bf58d882c4cbaf7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "711c01a605a34ed9b15aa0aea6225d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SETTING UP GPU**"
      ],
      "metadata": {
        "id": "mrbNDkuY2FdN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48BU3sqMmmgK",
        "outputId": "b9e9d547-ac4a-4197-f25e-98ad2dc1807d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "bf6lbNvTsfnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c664506-bf4f-47fc-ad06-1bc98eabec21"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSTALLING DEPENDENCIES**"
      ],
      "metadata": {
        "id": "_cKhyvMd21_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install wget"
      ],
      "metadata": {
        "id": "vtUPXQeH24BU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "550ed461-1623-4e49-8fbb-01c256fd7fb8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DOWNLOADING DATASET**"
      ],
      "metadata": {
        "id": "owivv6175v4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ],
      "metadata": {
        "id": "J3tys5_47lQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f04f4869-6dde-490b-9b4a-b0ac3457c0bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the dataset (if we haven't already)\n",
        "if not os.path.exists('./cola_public/'):\n",
        "  !unzip cola_public_1.1.zip"
      ],
      "metadata": {
        "id": "qugldWKM7lXj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAVERSING DATASET**"
      ],
      "metadata": {
        "id": "cp6BBBj_50VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "metadata": {
        "id": "NjCL-qf_84tF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "1f35b782-abbf-4786-f3f7-59f710cf0c6f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     sentence_source  label label_notes  \\\n",
              "7388           sks13      0           *   \n",
              "7436           sks13      1         NaN   \n",
              "1685            r-67      0           *   \n",
              "4067            ks08      0           *   \n",
              "1422            r-67      1         NaN   \n",
              "6165            c_13      1         NaN   \n",
              "8409            ad03      1         NaN   \n",
              "7531           sks13      1         NaN   \n",
              "4330            ks08      0           *   \n",
              "6442            d_98      0           *   \n",
              "\n",
              "                                               sentence  \n",
              "7388                             I saw the physics one.  \n",
              "7436  John will have been eating cake enthusiastically.  \n",
              "1685             The one whose I stole bike was John's.  \n",
              "4067                       That John coughs loves Bill.  \n",
              "1422             The boy's uncle and aunt were kissing.  \n",
              "6165                   What did Stacy say Becky bought?  \n",
              "8409                                    The men chuckle  \n",
              "7531  I noticed John's excessive appreciation of him...  \n",
              "4330                          They promised it to rain.  \n",
              "6442                  You must pick any of the flowers.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f188fd5-57c3-440e-b661-178017980537\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7388</th>\n",
              "      <td>sks13</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>I saw the physics one.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7436</th>\n",
              "      <td>sks13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>John will have been eating cake enthusiastically.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1685</th>\n",
              "      <td>r-67</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>The one whose I stole bike was John's.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4067</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>That John coughs loves Bill.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1422</th>\n",
              "      <td>r-67</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The boy's uncle and aunt were kissing.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6165</th>\n",
              "      <td>c_13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What did Stacy say Becky bought?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8409</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The men chuckle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7531</th>\n",
              "      <td>sks13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I noticed John's excessive appreciation of him...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4330</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>They promised it to rain.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6442</th>\n",
              "      <td>d_98</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>You must pick any of the flowers.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f188fd5-57c3-440e-b661-178017980537')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f188fd5-57c3-440e-b661-178017980537 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f188fd5-57c3-440e-b661-178017980537');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "metadata": {
        "id": "W4aYkmyy88z0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOADING BERT TOCKENIZER**"
      ],
      "metadata": {
        "id": "FloOXRPP9Eue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)"
      ],
      "metadata": {
        "id": "S7AgLvml9Dx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e1f76fc-bb38-4dd3-e7b7-2e21501a6efa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "metadata": {
        "id": "PYKbEJu79lIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4e9b8ef-9dc5-4963-e2eb-a235bfa16821"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['[UNK]', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [100, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "metadata": {
        "id": "oT-JrHeI9lUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d54f63-4b8d-4cd3-d1c3-9fb9be0c2872"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs: tensor([  101,   100,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,\n",
            "         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPLITTING AND LOADING DATASET**"
      ],
      "metadata": {
        "id": "KRWPH0ua-Ndy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 85-15 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.85 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "metadata": {
        "id": "F_g9MgO_-Zvb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd5fc122-0853-43dc-941b-2d607c637fb1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7,268 training samples\n",
            "1,283 validation samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "metadata": {
        "id": "11FY-IjS-aAj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CREATING BERT MODEL**"
      ],
      "metadata": {
        "id": "iTKxGj1h-VQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "3WX9qrBc-wvS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9564d91b31584ebfbc3f304de49b3649",
            "1005db29a16544bca890947e2cdda77a",
            "7dfddb72204545ca809d76db9cb3cca2",
            "d39864d43a6a4a0eb5c698eb79b0e45c",
            "a331b1142d004d2ca8d8cbf49603fd33",
            "3632c3540f6f4f398387a675efaf5aed",
            "bfc279b71e2c4eedbec2b59abac9f968",
            "481f63f6c7ee4c719e0760c93a3d3bcb",
            "2b07367a3ee240c7a92f372d55d8ee92",
            "3b0f81ab4a4348d7bf58d882c4cbaf7a",
            "711c01a605a34ed9b15aa0aea6225d7a"
          ]
        },
        "outputId": "b260f087-fc16-4d2f-c705-075c9a5a57cb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9564d91b31584ebfbc3f304de49b3649"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "metadata": {
        "id": "Wfxxc6c9-xpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdbe9efd-0178-4d02-8a86-f335814d006a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SETTING UP OPTIMIZER AND SCHEDULER**"
      ],
      "metadata": {
        "id": "VkW6ydH0-45s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "metadata": {
        "id": "6ZCh_3Mi_uu6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbab652a-cbd6-4946-be71-0c4491d24305"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "HHsRgR4f_z0x"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAINING BERT MODEL**"
      ],
      "metadata": {
        "id": "-_LEse-kAwe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "jpgfZ1sqAoOx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''Takes a time in seconds and returns a string hh:mm:ss'''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "CFRUfu8mAqY6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        p= model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "        loss=p[0]\n",
        "        logits=p[1]\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            q = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            loss=q[0]\n",
        "            logits=q[1]\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "metadata": {
        "id": "8wGCqJ60As9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a276a33-0914-49c5-b4a7-1b85418fe9fe"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    228.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    228.    Elapsed: 0:00:26.\n",
            "  Batch   120  of    228.    Elapsed: 0:00:40.\n",
            "  Batch   160  of    228.    Elapsed: 0:00:53.\n",
            "  Batch   200  of    228.    Elapsed: 0:01:07.\n",
            "\n",
            "  Average training loss: 0.23\n",
            "  Training epoch took: 0:01:16\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.54\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    228.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    228.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    228.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    228.    Elapsed: 0:00:55.\n",
            "  Batch   200  of    228.    Elapsed: 0:01:09.\n",
            "\n",
            "  Average training loss: 0.24\n",
            "  Training epoch took: 0:01:18\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.54\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    228.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    228.    Elapsed: 0:00:28.\n",
            "  Batch   120  of    228.    Elapsed: 0:00:42.\n",
            "  Batch   160  of    228.    Elapsed: 0:00:56.\n",
            "  Batch   200  of    228.    Elapsed: 0:01:10.\n",
            "\n",
            "  Average training loss: 0.19\n",
            "  Training epoch took: 0:01:19\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.62\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    228.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    228.    Elapsed: 0:00:28.\n",
            "  Batch   120  of    228.    Elapsed: 0:00:42.\n",
            "  Batch   160  of    228.    Elapsed: 0:00:56.\n",
            "  Batch   200  of    228.    Elapsed: 0:01:10.\n",
            "\n",
            "  Average training loss: 0.18\n",
            "  Training epoch took: 0:01:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.62\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:05:32 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VISUALIZING PERFORMANCE PARAMETERS**"
      ],
      "metadata": {
        "id": "or-IqfZeID6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "metadata": {
        "id": "tCRPdZaxH_pW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "baf04d7e-9e9d-4d1d-e9c6-bc1ac28f5f31"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.23         0.54           0.81       0:01:16         0:00:05\n",
              "2               0.24         0.54           0.80       0:01:18         0:00:05\n",
              "3               0.19         0.62           0.81       0:01:19         0:00:05\n",
              "4               0.18         0.62           0.81       0:01:20         0:00:05"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ff69c22-96f0-4793-af0a-f655dd56c5ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0:01:16</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0:01:18</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.19</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0:01:19</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.18</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0:01:20</td>\n",
              "      <td>0:00:05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ff69c22-96f0-4793-af0a-f655dd56c5ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9ff69c22-96f0-4793-af0a-f655dd56c5ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9ff69c22-96f0-4793-af0a-f655dd56c5ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t-lzzTXpICwc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "c19cef9e-c225-4368-d7d1-b2aa1dfa2d18"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Mj_YkpvsKu1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f6172d0-29ad-44a7-83c0-0b89b6749132"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "\n",
        "print('--------------------------------DONE--------------------------------')"
      ],
      "metadata": {
        "id": "qkBxRLf3K43r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5180561-e2d6-4cfd-e3a6-fb35f12364d2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "--------------------------------DONE--------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)\n",
        "\n",
        "# Create a barplot showing the MCC score for each batch of test samples.\n",
        "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
        "plt.title('MCC Score per Batch')\n",
        "plt.ylabel('MCC Score (-1 to +1)')\n",
        "plt.xlabel('Batch #')\n",
        "plt.show()\n",
        "\n",
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "print('Total MCC: %.3f' % mcc)"
      ],
      "metadata": {
        "id": "ZvFI7kM7LSM-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "a1ffe4f4-798c-4000-f9a3-7fb9643b61fb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total MCC: 0.426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVING MODEL**"
      ],
      "metadata": {
        "id": "BNK725IOIo-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdM2f9E-OvMK",
        "outputId": "433cd980-da66-45c8-8a5e-c16a5dc245e4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "output_dir = '/content/gdrive/MyDrive/Colab Notebooks/NLP'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "LLY8Tm6dITf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "217e9532-0a49-47fc-d4e0-f04f5cdfb7dc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to /content/gdrive/MyDrive/Colab Notebooks/NLP\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/gdrive/MyDrive/Colab Notebooks/NLP/tokenizer_config.json',\n",
              " '/content/gdrive/MyDrive/Colab Notebooks/NLP/special_tokens_map.json',\n",
              " '/content/gdrive/MyDrive/Colab Notebooks/NLP/vocab.txt',\n",
              " '/content/gdrive/MyDrive/Colab Notebooks/NLP/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
        "print('Loading BERT model...')\n",
        "model_loaded = BertForSequenceClassification.from_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "l9_B7s1mIdH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5851b92a-1580-4454-c29f-1096c21b4d85"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n",
            "Loading BERT model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREDICTING SENTENCES**"
      ],
      "metadata": {
        "id": "4FbSGNxcJ6lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check it for a given sentence\n",
        "sent = \"He goes to school\"\n",
        "encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "# Add the encoded sentence to the list.    \n",
        "input_id = encoded_dict['input_ids']\n",
        "    \n",
        "# And its attention mask (simply differentiates padding from non-padding).\n",
        "attention_mask = encoded_dict['attention_mask']\n",
        "input_id = torch.LongTensor(input_id)\n",
        "attention_mask = torch.LongTensor(attention_mask)"
      ],
      "metadata": {
        "id": "ZcMhFXZVJ1Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  # Forward pass, calculate logit predictions\n",
        "  outputs = model_loaded(input_id, token_type_ids=None, attention_mask=attention_mask)\n",
        "\n",
        "logits = outputs[0]\n",
        "index = logits.argmax()\n",
        "if index == 1:\n",
        "  print(\"Gramatically correct\")\n",
        "else:\n",
        "  print(\"Gramatically in-correct\")"
      ],
      "metadata": {
        "id": "pKkp4dXAKTSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d87b829-31de-43d0-9834-350b273b527a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gramatically correct\n"
          ]
        }
      ]
    }
  ]
}